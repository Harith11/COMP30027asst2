{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6a3030-5494-44d1-9dfd-1f259e8102c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'processed_train.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95a3889-9481-4584-a9b7-1d5dd0789b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hkharith/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.610648918469218, 0.4630333741285186, 0.37289210162762565, 0.610648918469218)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Define predictor variables (X) and the target variable (y)\n",
    "X = df.drop('imdb_score_binned', axis=1)\n",
    "y = df['imdb_score_binned']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize and train the dummy classifier (0R)\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the dummy classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate additional metrics: f1 score, precision, and recall\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa30459-7a0e-4d5f-baab-94f4536700ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/hkharith/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/hkharith/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/hkharith/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost #remove comment to install if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d48f76-c59c-486e-814c-83cf76f083e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hkharith/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hkharith/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hkharith/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hkharith/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hkharith/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7350221852468108,\n",
       " 'f1': 0.7001067241805645,\n",
       " 'precision': 0.7134487345261303,\n",
       " 'recall': 0.7350221852468108}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Define the stratified K-fold cross-validator\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(random_state=1, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {'accuracy': make_scorer(accuracy_score),\n",
    "                   'f1': make_scorer(f1_score, average='weighted'),\n",
    "                   'precision': make_scorer(precision_score, average='weighted'),\n",
    "                   'recall': make_scorer(recall_score, average='weighted')}\n",
    "\n",
    "# Perform stratified K-fold cross-validation and calculate metrics\n",
    "cross_val_metrics = {metric: cross_val_score(xgb_clf, X, y, cv=stratified_kfold, scoring=scorer)\n",
    "                     for metric, scorer in scoring_metrics.items()}\n",
    "\n",
    "# Calculate and display the average of each metric\n",
    "average_metrics = {metric: np.mean(scores) for metric, scores in cross_val_metrics.items()}\n",
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb80d5e-056b-4217-ae90-b1d551d3f432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object with stratified K-fold\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=stratified_kfold, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best score and parameters\n",
    "best_score = grid_search.best_score_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "(best_score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc952df4-72ef-4030-a058-c23181e67970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7437603993344426,\n",
       " 0.7720465890183028,\n",
       " 0.7371048252911814,\n",
       " 0.7138103161397671,\n",
       " 0.7533333333333333]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './processed_train.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop('imdb_score_binned', axis=1)\n",
    "y = data['imdb_score_binned']\n",
    "\n",
    "# Define the model with hyperparameters\n",
    "model = XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9)\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "fold_performance = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    fold_performance.append(score)\n",
    "\n",
    "# Output the performance\n",
    "fold_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b1cd4d1-1e7c-4a2a-b837-d12b72b33a77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8861517976031957"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the full dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "full_dataset_score = model.score(X, y)\n",
    "\n",
    "full_dataset_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68ecf381-24e6-4023-b524-385242c1b812",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_score_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  predicted_score_binned\n",
       "0   1                       2\n",
       "1   2                       2\n",
       "2   3                       2\n",
       "3   4                       3\n",
       "4   5                       2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "file_path_test = 'processed_test_aligned.csv'\n",
    "test_data = pd.read_csv(file_path_test)\n",
    "\n",
    "# Prepare features for prediction\n",
    "X_test = test_data.drop('imdb_score_binned', axis=1)\n",
    "\n",
    "# Predict on the test dataset\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Add predictions to the test dataframe\n",
    "test_data['predicted_score_binned'] = predictions\n",
    "\n",
    "# Output the first few rows of the test dataframe with predictions\n",
    "test_data[['id', 'predicted_score_binned']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff528f5e-4b6e-4f66-8c3d-fb1748cf2019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_output = test_data[['id', 'predicted_score_binned']]\n",
    "predicted_output.columns = ['id', 'imdb_score_binned']\n",
    "predicted_output.head()\n",
    "predicted_output.to_csv('xgbscorefin.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92777bf-a684-4913-8072-37f851ab44cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure_factory\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mff\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the confusion matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y, y_pred_full)\n\u001b[1;32m      7\u001b[0m fig \u001b[38;5;241m=\u001b[39m ff\u001b[38;5;241m.\u001b[39mcreate_annotated_heatmap(\n\u001b[1;32m      8\u001b[0m     z\u001b[38;5;241m=\u001b[39mconf_matrix,\n\u001b[1;32m      9\u001b[0m     x\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(conf_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     showscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[1;32m     16\u001b[0m     autosize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     yaxis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred_full)\n",
    "\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=conf_matrix,\n",
    "    x=[str(i) for i in range(conf_matrix.shape[1])],\n",
    "    y=[str(i) for i in range(conf_matrix.shape[0])],\n",
    "    colorscale='Viridis',\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=500,  \n",
    "    height=500,  \n",
    "    xaxis=dict(title='Predicted label'),\n",
    "    yaxis=dict(title='Actual label')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e873c8-b11e-4af0-8732-319efaba5308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y, y_pred_full)\n\u001b[1;32m      5\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y, y_pred_full, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y, y_pred_full, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred_full)\n",
    "precision = precision_score(y, y_pred_full, average='weighted')\n",
    "recall = recall_score(y, y_pred_full, average='weighted')\n",
    "f1 = f1_score(y, y_pred_full, average='weighted')\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf941e63-a723-4bd1-b5cc-29f0702dc74e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the number of training examples to use for learning curve\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Calculate the learning curve\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    estimator=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean, name='Training score',\n",
    "                         mode='lines+markers', line=dict(color='blue')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=train_sizes, y=validation_scores_mean, name='Cross-validation score',\n",
    "                         mode='lines+markers', line=dict(color='red')))\n",
    "\n",
    "fig.update_layout(xaxis_title='Training examples', yaxis_title='Score',\n",
    "                  legend=dict(yanchor='top', y=0.99, xanchor='right', x=1.2),\n",
    "                  autosize=False, width=600, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ccc140-ad44-46c1-be73-1054c74f7277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Define the range of max_depth values\n",
    "param_range = np.arange(3, 7)\n",
    "\n",
    "# Calculate validation curve\n",
    "train_scores, test_scores = validation_curve(\n",
    "    model, X, y, param_name='max_depth', param_range=param_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot validation curve\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "# Add the training score curve\n",
    "fig.add_trace(go.Scatter(x=param_range, y=train_mean, name='Training score',\n",
    "                         mode='lines+markers', line_color='blue'),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Add the test score curve\n",
    "fig.add_trace(go.Scatter(x=param_range, y=test_mean, name='Cross-validation score',\n",
    "                         mode='lines+markers', line_color='orange'),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text='max_depth', row=1, col=1)\n",
    "# Update yaxis properties\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=1)\n",
    "# Update layout to make the plot square\n",
    "fig.update_layout(autosize=False, width=600, height=600)  # Adjust width and height to make the plot square\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f988e55-0328-461b-a47a-d7ccd40170f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
